{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emotion Classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small parts of this were taken from the Notebook [here](https://colab.research.google.com/drive/1nwCE6b9PXIKhv2hvbqf1oZKIGkXMTi1X#scrollTo=t23zHggkEpc-), see roberta_emotion_class.ipynb for further info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users1/zabereus/zabereus/corpora_env/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, fasttext, os\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from huggingface_hub import hf_hub_download\n",
    "from torchtext.data import get_tokenizer\n",
    "import pytorch_lightning as pl\n",
    "from argparse import Namespace\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"./data\"\n",
    "os.environ[\"HF_MODELS_CACHE\"] = \"./model\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"./model\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emotion labels\n",
    "label2int = {\n",
    "  \"sadness\": 0,\n",
    "  \"joy\": 1,\n",
    "  \"love\": 2,\n",
    "  \"anger\": 3,\n",
    "  \"fear\": 4,\n",
    "  \"surprise\": 5\n",
    "}\n",
    "\n",
    "emotions = [ \"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ft/models--facebook--fasttext-en-vectors/snapshots/a80392390daaee1a91000da45c376d512e1dc555/model.bin\n"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-en-vectors\", filename=\"model.bin\", cache_dir=\"./ft\")\n",
    "print(model_path)\n",
    "embedding_model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizersCollateFn:\n",
    "    def __init__(self, max_tokens=512):\n",
    "        #self.tokenizer = tokenize.NLTKWordTokenizer()\n",
    "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        sequences = [self.tokenizer(x[0]) for x in batch]\n",
    "        embedded_sequences = [torch.tensor(np.array([embedding_model[word] for word in s])) for s in sequences]\n",
    "        packed_sequences = pack_sequence(embedded_sequences, enforce_sorted=False)\n",
    "        labels = torch.tensor([x[1] for x in batch])\n",
    "        #print(\"packed sequence in collate\", packed_sequences.batch_sizes[0])\n",
    "\n",
    "        return packed_sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "class EmotionDataModule(pl.LightningDataModule):\n",
    "    def setup(self, stage):\n",
    "        train = load_dataset(\"dair-ai/emotion\")[\"train\"]\n",
    "        self.train_dataset = [(ex['text'], ex['label']) for ex in train]\n",
    "        val = load_dataset(\"dair-ai/emotion\")[\"validation\"]\n",
    "        self.val_dataset = [(ex['text'], ex['label']) for ex in val]\n",
    "        test = load_dataset(\"dair-ai/emotion\")[\"test\"]\n",
    "        self.test_dataset = [(ex['text'], ex['label']) for ex in test]\n",
    "        \n",
    "    def train_dataloader(self):    \n",
    "        return DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                    collate_fn=TokenizersCollateFn(), pin_memory=True, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                    collate_fn=TokenizersCollateFn())\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                    collate_fn=TokenizersCollateFn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/digantamisra98/Mish/blob/b5f006660ac0b4c46e2c6958ad0301d7f9c59651/Mish/Torch/mish.py\n",
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEmotionModel(nn.Module):\n",
    "    def __init__(self, n_classes, embedding_size=300, hidden_size=128, dropout=0.05):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            Mish(),\n",
    "            nn.Linear(64, n_classes)\n",
    "        )\n",
    "\n",
    "        # for layer in self.classifier:\n",
    "        #     if isinstance(layer, nn.Linear):\n",
    "        #         layer.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        #         if layer.bias is not None:\n",
    "        #             layer.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_, *args):\n",
    "        X = input_\n",
    "        y, _ = self.rnn(X)\n",
    "        # output is shape (N,L,Dâˆ—H_out)\n",
    "        y, lengths = pad_packed_sequence(y, batch_first=True)\n",
    "        #print(\"after padding: \", y.size())\n",
    "        #y = y[:, 0, :] # last hidden state\n",
    "        y = torch.stack([y[i, lengths[i]-1, :] for i in range(len(lengths))])\n",
    "        #print(\"after taking last: \", y.size())\n",
    "        # for loop\n",
    "        y = self.classifier(y)\n",
    "        #print(\"after linear: \", y.size())\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting the model together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEmotionClassifier(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.model = RNNEmotionModel(len(emotions))\n",
    "        self.loss = nn.CrossEntropyLoss() ## combines LogSoftmax() and NLLLoss()\n",
    "        #self.hparams = hparams\n",
    "        self.hparams.update(vars(hparams))\n",
    "        self.training_step_outputs = []\n",
    "        self.max_val_acc = 0\n",
    "\n",
    "    def step(self, batch, step_name=\"train\"):\n",
    "        self.train()\n",
    "        X, y = batch\n",
    "        loss = self.loss(self.forward(X), y)\n",
    "        #print(loss)\n",
    "        self.training_step_outputs.append(loss)\n",
    "        loss_key = f\"{step_name}_loss\"\n",
    "        tensorboard_logs = {loss_key: loss}\n",
    "        #self.log(loss_key, loss, batch_size=batch_size)\n",
    "\n",
    "        return { (\"loss\" if step_name == \"train\" else loss_key): loss, 'log': tensorboard_logs,\n",
    "               \"progress_bar\": {loss_key: loss}}\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        return self.model(X, *args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #print(batch[0])\n",
    "        return self.step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"val\")\n",
    "\n",
    "    def validation_end(self, outputs: List[dict]):\n",
    "        loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch, \"test\")\n",
    "\n",
    "    #@lru_cache()\n",
    "    def total_steps(self):\n",
    "        return self.hparams.data_size // self.hparams.accumulate_grad_batches * self.hparams.epochs\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=self.hparams.warmup_steps,\n",
    "                    num_training_steps=self.total_steps(),\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": lr_scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCallback(pl.Callback):\n",
    "    def on_train_epoch_end(self, trainer, model):\n",
    "        if trainer.current_epoch % 4 == 0 and trainer.current_epoch > 14:\n",
    "            epoch_mean = torch.stack(model.training_step_outputs).mean()\n",
    "            print(epoch_mean.item(), \"\\n\")\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                model.cuda()\n",
    "                \n",
    "                true_y, pred_y = [], []\n",
    "                for i, batch_ in enumerate(trainer.datamodule.train_dataloader()):\n",
    "                    X, y = batch_\n",
    "                    batch = X.cuda()\n",
    "                    y_pred = torch.argmax(model(batch), dim=1)\n",
    "                    true_y.extend(y.cpu())\n",
    "                    pred_y.extend(y_pred.cpu())\n",
    "                correct = (torch.tensor(true_y) == torch.tensor(pred_y)).float().sum() / len(true_y)\n",
    "                print(\"on train: \", correct.item())\n",
    "                \n",
    "                true_y, pred_y = [], []\n",
    "                for i, batch_ in enumerate(trainer.datamodule.val_dataloader()):\n",
    "                    X, y = batch_\n",
    "                    batch = X.cuda()\n",
    "                    y_pred = torch.argmax(model(batch), dim=1)\n",
    "                    true_y.extend(y.cpu())\n",
    "                    pred_y.extend(y_pred.cpu())\n",
    "                correct = (torch.tensor(true_y) == torch.tensor(pred_y)).float().sum() / len(true_y)\n",
    "                print(\"on dev: \", correct.item())\n",
    "                if correct.item() > model.max_val_acc and correct.item() > 0.66:\n",
    "                    torch.save(model.state_dict(), \"model/rnn\")\n",
    "                    model.max_val_acc = correct.item()\n",
    "                    print(\"model saved at \", correct.item())\n",
    "        model.training_step_outputs.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Emotion Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Namespace(\n",
    "    batch_size=batch_size,\n",
    "    warmup_steps=100,\n",
    "    epochs=100,\n",
    "    lr=2e-4,\n",
    "    accumulate_grad_batches=1,\n",
    "    data_size=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## garbage collection\n",
    "import gc; gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/users1/zabereus/zabereus/corpora_env/lib64/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "model = RNNEmotionClassifier(hparams)\n",
    "data_module = EmotionDataModule()\n",
    "trainer = pl.Trainer(max_epochs=hparams.epochs, accumulate_grad_batches=hparams.accumulate_grad_batches, enable_progress_bar=True,\n",
    "                     callbacks=[LossCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users1/zabereus/zabereus/corpora_env/lib64/python3.12/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def print_eval(dataloader, save_misclass=False):\n",
    "    with torch.no_grad():\n",
    "        progress = [\"/\", \"-\", \"\\\\\", \"|\", \"/\", \"-\", \"\\\\\", \"|\"]\n",
    "        model = RNNEmotionClassifier(hparams)\n",
    "        model.load_state_dict(torch.load(\"model/rnn\"))\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "        true_y, pred_y = [], []\n",
    "        for i, batch_ in enumerate(dataloader):\n",
    "            X, y = batch_\n",
    "            batch = X.cuda()\n",
    "            print(progress[i % len(progress)], end=\"\\r\")\n",
    "            y_pred = torch.argmax(model(batch), dim=1)\n",
    "            true_y.extend(y.cpu())\n",
    "            pred_y.extend(y_pred.cpu())\n",
    "    print(\"\\n\" + \"_\" * 80)\n",
    "    print(classification_report(true_y, pred_y, target_names=label2int.keys(), digits=len(emotions)))\n",
    "    print(confusion_matrix(true_y, pred_y, labels=label2int.keys()))\n",
    "\n",
    "    if save_misclass:\n",
    "        with open(\"misclass_rnn.txt\", \"w\") as f:\n",
    "            for i in range(len(y_pred)):\n",
    "                if true_y[i] != pred_y[i]:  \n",
    "                    line = \"{} true label: {}, predicted label: {}\".format(data_module.test_dataset[i][0], true_y[i], pred_y[i])\n",
    "                    f.write(line)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eval(data_module.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 7894 entries\n",
      "\\\n",
      "________________________________________________________________________________\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness   0.464012  0.457014  0.460486      1326\n",
      "         joy   0.362676  0.715775  0.481421      1439\n",
      "        love   0.739884  0.063777  0.117431      2007\n",
      "       anger   0.360179  0.418454  0.387136      1539\n",
      "        fear   0.192257  0.524150  0.281325       559\n",
      "    surprise   0.410646  0.105469  0.167832      1024\n",
      "\n",
      "    accuracy                       0.355840      7894\n",
      "   macro avg   0.421609  0.380773  0.315939      7894\n",
      "weighted avg   0.469269  0.355840  0.312133      7894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dataset = load_dataset(\"go_emotions\", \"simplified\")\n",
    "class GoEmotionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = load_dataset(\"go_emotions\", \"simplified\")\n",
    "        self.go_dataset = []\n",
    "        # surprise: 26, sadness 25, joy 17, love 18, anger 2, fear 14\n",
    "        mapping_dict = {25:0, 17:1, 18:2, 2:3, 14:4, 26:5}\n",
    "        for ex in data['train']:#.extend(data['test']).extend(data['validation']):\n",
    "            labels = set(ex['labels']).intersection(mapping_dict.keys())\n",
    "            if labels:\n",
    "                text = re.sub(r'[^\\w\\s]', '', ex['text'].lower()) # remove punctuation, make lowercase\n",
    "                self.go_dataset.append((text, mapping_dict[list(labels)[0]]))\n",
    "        print(f\"Dataset loaded: {len(self.go_dataset)} entries\")\n",
    "        \n",
    "    def go_dataloader(self):    \n",
    "        return DataLoader(self.go_dataset, batch_size=batch_size, shuffle=True,\n",
    "                    collate_fn=TokenizersCollateFn(), pin_memory=True, num_workers=4)\n",
    "\n",
    "go_dm = GoEmotionDataModule()\n",
    "    \n",
    "\n",
    "print_eval(go_dm.go_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corpora_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
